/*
 * Copyright 2002 by Hans Reiser, licensing governed by reiser4/README
 */

#include "reiser4.h"

/*
 *  Block number allocation and free space counting in reiser4 are done in two
 * stages: first, we assign special block numbers to just created nodes and
 * subtracts a number of that nodes from free blocks counter . Those special
 * numbers have nothing common with real block numbers on a disk device and
 * they are called "fake" in reiser4. Actually only formatted nodes require
 * those numbers because we need something to be inserted into internal nodes
 * and, thus, to support tree operations: lookup and variants of tree
 * modifications. Fake block numbers should be replaced by real "on-disk"
 * block numbers before time we write data to disk. It is a second stage of
 * block numbers allocation.
 *
 * Current implementation of reiser4 uses 64-bit integers for block
 * numbers. We use highest bit in 64-bit block number to distinguish fake and
 * real block numbers. So, only 63 bits may be used to addressing of real
 * device blocks. That "fake" block numbers space is divided into subspaces of
 * fake block numbers for data blocks and for shadow (working) bitmap
 * blocks. Fake block numbers for data blocks are generated by a cyclic
 * counter, which gets incremented after each real block allocation. We assume
 * that it is impossible to overload this counter during one transaction life.
 */


/* Initialize a blocknr hint. */

void blocknr_hint_init (reiser4_blocknr_hint *hint)
{
	xmemset (hint, 0, sizeof (reiser4_blocknr_hint));
}

/* Release any resources of a blocknr hint. */
void blocknr_hint_done (reiser4_blocknr_hint *hint UNUSED_ARG)
{
	/* FIXME: relase bitmap lock. */
}

/** is it a real block number from real block device or fake block number for
 * not-yet-mapped object? */
/* Audited by: green(2002.06.11) */
int blocknr_is_fake(const reiser4_block_nr * da)
{
	/* The reason for not simply returning result of '&' operation is that
	   while return value is (possibly 32bit) int,  the reiser4_block_nr is
	   at least 64 bits long, and high bit (which is the only possible
	   non zero bit after the masking) would be stripped off */ 
	return (*da & REISER4_FAKE_BLOCKNR_BIT_MASK) ? 1 : 0;
}

/* Static functions for <reiser4 super block>/<reiser4 context> block counters
 * arithmetic. Mostly, they are isolated to not to code same assertions in
 * several places. */

static void add_to_sb_grabbed (const struct super_block * super, __u64 count)
{
	__u64 grabbed_blocks = reiser4_grabbed_blocks (super);

	grabbed_blocks += count;
	reiser4_set_grabbed_blocks (super, grabbed_blocks);
}

static void sub_from_sb_grabbed (const struct super_block * super, __u64 count)
{
	__u64 grabbed_blocks = reiser4_grabbed_blocks (super);

	assert ("zam-525", grabbed_blocks >= count);
	grabbed_blocks -= count;
	reiser4_set_grabbed_blocks (super, grabbed_blocks);
}

static void add_to_ctx_grabbed (__u64 count)
{
	reiser4_context * ctx = get_current_context();
	ctx->grabbed_blocks += count;
}

static void sub_from_ctx_grabbed (__u64 count)
{
	reiser4_context * ctx = get_current_context();

	assert ("zam-527", ctx->grabbed_blocks >= count);
	ctx->grabbed_blocks -= count;
}

static void add_to_sb_unallocated (const struct super_block * super, __u64 count)
{
	__u64 unallocated = reiser4_unallocated_blocks(super);

	unallocated += count;
	reiser4_set_unallocated_blocks(super, unallocated);
}

static void sub_from_sb_unallocated (const struct super_block * super, __u64 count)
{
	__u64 unallocated = reiser4_unallocated_blocks(super);

	assert ("zam-528", unallocated >= count);
	unallocated -= count;
	reiser4_set_unallocated_blocks(super, unallocated);
	
}

static void add_to_sb_used (const struct super_block * super, __u64 count)
{
	__u64 used_blocks = reiser4_data_blocks(super);
	used_blocks += count;
	reiser4_set_data_blocks (super, used_blocks);
}

static void sub_from_sb_used (const struct super_block *super, __u64 count)
{
	__u64 used_blocks = reiser4_data_blocks(super);

	assert ("zam-530", used_blocks >= count);
	used_blocks -= count;
	reiser4_set_data_blocks (super, used_blocks);
}

/**
 * should be called after @count fake block numbers are allocated or
 * unallocated extent (@count blocks in size) created
 */
void reiser4_count_fake_allocation (__u64 count)
{
	const struct super_block * super = reiser4_get_current_sb(); 

	sub_from_ctx_grabbed (count);

	reiser4_spin_lock_sb(super);

	sub_from_sb_grabbed(super, count);
	add_to_sb_unallocated(super, count); 

	reiser4_spin_unlock_sb (super);
}

/**
 * disk space, virtually used by fake block numbers is counted as "grabbed" again.
 */
void reiser4_count_fake_deallocation (__u64 count)
{
	const struct super_block * super = reiser4_get_current_sb(); 

	add_to_ctx_grabbed (count);

	reiser4_spin_lock_sb(super);

	add_to_sb_grabbed(super, count);
	sub_from_sb_unallocated(super, count); 

	reiser4_spin_unlock_sb (super);
}

/**
 * adjust sb block counters when @count unallocated blocks get mapped to disk
 */
void reiser4_count_block_mapping (__u64 count)
{
	const struct super_block * super = reiser4_get_current_sb(); 

	reiser4_spin_lock_sb(super);

	sub_from_sb_unallocated(super, count);
	add_to_sb_used(super, count);

	reiser4_spin_unlock_sb (super);
}
/**
 * adjust sb block counters when @count unallocated blocks get unmapped from
 * disk
 */
void reiser4_count_block_unmapping (__u64 count)
{
	const struct super_block * super = reiser4_get_current_sb(); 

	reiser4_spin_lock_sb(super);

	add_to_sb_unallocated(super, count);
	sub_from_sb_used(super, count);

	reiser4_spin_unlock_sb (super);
}

/**
 * adjust sb block counters, if real (on-disk) block allocation immediately
 * follows grabbing of free disk space.
 */
void reiser4_count_real_allocation (__u64 count)
{
	const struct super_block * super = reiser4_get_current_sb(); 

	sub_from_ctx_grabbed(count);

	reiser4_spin_lock_sb(super);

	sub_from_sb_grabbed(super, count);
	add_to_sb_used(super, count);

	reiser4_spin_unlock_sb (super);
}

/**
 * adjust sb block counters if real (on-disk) blocks do not become unallocated
 * after freeing, @count blocks become "grabbed".
 */
void reiser4_count_real_deallocation (__u64 count)
{
	const struct super_block * super = reiser4_get_current_sb(); 

	add_to_ctx_grabbed(count);

	reiser4_spin_lock_sb(super);

	add_to_sb_grabbed(super, count);
	sub_from_sb_used(super, count);

	reiser4_spin_unlock_sb (super);
}

/* Adjust "working" free blocks counter for number of blocks we are going to
 * allocate.  Record number of grabbed blocks in fs-wide and per-thread
 * counters.  This function should be called before bitmap scanning or
 * allocating fake block numbers
 *
 * @super           -- pointer on reiser4 super block;
 * @min_block_count -- minimum number of blocks we reserve;
 * @max_block_count -- maximum number of blocks we want to reserve;
 * @reserved        -- out parameter for max. number of reserved blocks, 
 *                     less than @max_block_count and
 *                     more than or equal to @min_block_count;
 * @return          -- 0 if success,  -ENOSPC, if all
 *                     free blocks are preserved or already allocated.
 **/

/* FIXME-ZAM: reserved blocks could be counted in a reiser4 super block field,
 * it allows more error checks. */

int reiser4_grab_space (__u64 * grabbed, __u64 min_block_count, __u64 max_block_count)
{
	struct super_block * super = reiser4_get_current_sb ();
	__u64 free_blocks;
	int ret = 0;

	reiser4_spin_lock_sb (super);

	assert ("zam-472", grabbed != NULL);
	assert ("zam-473", min_block_count != 0);
	assert ("zam-474", max_block_count >= min_block_count);

	free_blocks = reiser4_free_blocks (super);

	if (free_blocks < min_block_count) {
		ret = -ENOSPC;
		goto unlock_and_ret;
	}

	*grabbed = free_blocks <= max_block_count ? free_blocks : max_block_count;

	add_to_ctx_grabbed(*grabbed);
	add_to_sb_grabbed(super, *grabbed);

	free_blocks -= *grabbed;
	reiser4_set_free_blocks (super, free_blocks);

 unlock_and_ret:
	reiser4_spin_unlock_sb (super);

	return ret;
} 

/**
 * Adjust free blocks count for blocks which were reserved but were not used.
 */
void reiser4_release_grabbed_space (__u64 count)
{
	struct super_block * super = reiser4_get_current_sb ();

	sub_from_ctx_grabbed(count);

	reiser4_spin_lock_sb (super);

	sub_from_sb_grabbed(super, count);
	reiser4_set_free_blocks (super, reiser4_free_blocks (super) + count);
	
	reiser4_spin_unlock_sb (super);
}
/**
 * release all grabbed blocks which where not used.
 */
void reiser4_release_all_grabbed_space (void)
{
	reiser4_context * ctx = get_current_context();
	__u64 grabbed = ctx->grabbed_blocks;

	reiser4_release_grabbed_space(grabbed);
}

/** a generator for tree nodes fake block numbers */
/* Audited by: green(2002.06.11) */
static void get_next_fake_blocknr (reiser4_block_nr *bnr)
{
	static spinlock_t       fake_lock = SPIN_LOCK_UNLOCKED;
	static reiser4_block_nr fake_gen  = 0;

	spin_lock (& fake_lock);
	*bnr = fake_gen++;
	spin_unlock (& fake_lock);
	
	*bnr &= ~REISER4_BLOCKNR_STATUS_BIT_MASK;
	*bnr |= REISER4_UNALLOCATED_STATUS_VALUE;

#if REISER4_DEBUG
	{
		znode * node;

		node = zlook(current_tree, bnr);
		assert ("zam-394", node == NULL);
	}
#endif
}


/* wrapper to call space allocation plugin */
int reiser4_alloc_blocks (reiser4_blocknr_hint *hint, reiser4_block_nr *blk,
			  reiser4_block_nr *len)
{
	space_allocator_plugin * splug;
	reiser4_block_nr needed = *len;
	block_stage_t stage = BLOCK_NOT_COUNTED;
	int ret;

	assert ("vs-514", (get_current_super_private () &&
			   get_current_super_private ()->space_plug &&
			   get_current_super_private ()->space_plug->alloc_blocks));

	if (hint != NULL) stage = hint->block_stage;

	if (stage == BLOCK_NOT_COUNTED) {
		ret = reiser4_grab_space (&needed, (reiser4_block_nr)1, *len);
		if (ret != 0) return ret;		
	}

	splug = get_current_super_private ()->space_plug;	
	ret = splug->alloc_blocks (get_space_allocator (reiser4_get_current_sb ()),
				   hint, (int) needed, blk, len);

	if (!ret) {

		switch (stage) {
		    case BLOCK_NOT_COUNTED:
			    reiser4_release_grabbed_space (needed - *len);
		    case BLOCK_GRABBED:
			    reiser4_count_real_allocation (*len);
			    break;
		    case BLOCK_UNALLOCATED:
			    reiser4_count_block_mapping (*len);
			    break;
		    default:
			    impossible ("zam-531", "wrong block stage");
		}
	} else {
		if (stage == BLOCK_NOT_COUNTED) 
			reiser4_release_grabbed_space (needed);
	}

	return ret;
}

/** Blocks deallocation function may do an actual deallocation through space
 * plugin allocation or store deleted block numbers in atom's delete_set data
 * structure depend on @defer parameter.
 */
int reiser4_dealloc_blocks (
	const reiser4_block_nr * start,
	const reiser4_block_nr * len,
	/* defer actual block freeing until transaction commit */
	int defer,
	/* if @defer is zero, @target_stage means the stage of blocks which
	 * will be deleted from WORKING bitmap. They might be just unmapped
	 * from disk, or freed but disk space is still grabbed by current
	 * thread, or these blocks must not be counted in any reiser4 sb block
	 * counters, see block_stage_t comment. */
	block_stage_t target_stage)
{
	txn_atom          * atom = NULL; 
	int                 ret;
	
	assert ("zam-431", *len != 0);
	assert ("zam-432", *start != 0);

	if (defer) {
		blocknr_set_entry * bsep = NULL;

		/* storing deleted block numbers in a blocknr set
		 * datastructure for further actual deletion */
		do {
			atom = get_current_atom_locked ();
			assert ("zam-430", atom != NULL);

			ret = blocknr_set_add_extent (atom, & atom->delete_set, &bsep, start, len);

			if (ret == -ENOMEM) {
				/* FIXME: JMACD->ZAM: return FAILURE. */
				/* FIXME: ZAM->JMACD: we need a reliable
				 * memory allocation for several things
				 * including this one. It is used in Linux
				 * kernel: see how block heads are
				 * allocated */
				return ret;
			}

			/* This loop might spin at most two times */
		} while (ret == -EAGAIN);

		assert ("zam-477", ret == 0);

		assert ("zam-433", atom != NULL);
		spin_unlock_atom (atom);

	} else {
		/* actual deletion is done through space allocator plugin */
		space_allocator_plugin * splug;

		assert ("zam-425", get_current_super_private () != NULL);

		splug = get_current_super_private() -> space_plug;

		assert ("zam-461", splug != NULL);
		assert ("zam-462", splug -> dealloc_blocks != NULL);

		splug->dealloc_blocks (get_space_allocator (reiser4_get_current_sb ()), *start, *len);

		switch (target_stage) {
		    case BLOCK_NOT_COUNTED:
			    reiser4_count_real_deallocation(*len);
			    reiser4_release_grabbed_space(*len);
			    break;
		    case BLOCK_GRABBED:
			    reiser4_count_real_deallocation(*len);
			    break;
		    case BLOCK_UNALLOCATED:
			    reiser4_count_block_unmapping(*len);
			    break;
		    default:
			    impossible ("zam-532", "wrong block stage");
		}
	}

	return 0;
}

/** obtain a block number for new formatted node which will be used to refer
 * to this newly allocated node until real allocation is done */
int assign_fake_blocknr (reiser4_block_nr *blocknr)
{
	int ret;
	reiser4_block_nr not_used;

#if 0
	ret = reiser4_grab_space (&not_used, (reiser4_block_nr)1, (reiser4_block_nr)1);

	if (ret != 0) return ret;

	get_next_fake_blocknr (blocknr);
	reiser4_count_fake_allocation((__u64)1);

	return 0;
#else
	if (1) {
		space_allocator_plugin * splug;
		reiser4_blocknr_hint preceder;
		reiser4_block_nr one;

		preceder.blk = 0;

		splug = get_current_super_private ()->space_plug;	
		return splug->alloc_blocks (get_space_allocator (reiser4_get_current_sb ()),
					    &preceder, 1, blocknr, &one);
	} else {
		get_next_fake_blocknr (blocknr);
		return 0;
	}

#endif;
}

/* release disk space (reserved or real one) depend on block number type (fake
 * or real) */
void release_blocknr (reiser4_block_nr * block)
{
	const reiser4_block_nr one = 1;

	assert ("zam-476", block != NULL);
	/* FIXME: more checks about jnode state should be here */

	if (blocknr_is_fake (block)) {
		reiser4_release_grabbed_space ((__u64)1);
	} else {
		reiser4_dealloc_blocks (block, &one, 1, BLOCK_NOT_COUNTED);
	}
}
/** wrappers for block allocator plugin methods */
extern void pre_commit_hook (void)
{
	space_allocator_plugin * splug;

	assert ("zam-502", get_current_super_private () != NULL);
	splug = get_current_super_private() -> space_plug;
	assert ("zam-503", splug != NULL);

	if (splug->pre_commit_hook != NULL) 
		splug->pre_commit_hook();
}

extern void post_commit_hook (void)
{
	space_allocator_plugin * splug;

	assert ("zam-504", get_current_super_private () != NULL);
	splug = get_current_super_private() -> space_plug;
	assert ("zam-505", splug != NULL);

	if (splug->post_commit_hook != NULL)
		splug->post_commit_hook();
}

extern void post_write_back_hook (void)
{
	space_allocator_plugin * splug;

	assert ("zam-504", get_current_super_private () != NULL);
	splug = get_current_super_private() -> space_plug;
	assert ("zam-505", splug != NULL);

	if (splug->post_commit_hook != NULL)
		splug->post_commit_hook();
}

/* 
 * Local variables:
 * c-indentation-style: "K&R"
 * mode-name: "LC"
 * c-basic-offset: 8
 * tab-width: 8
 * fill-column: 78
 * scroll-step: 1
 * End:
 */
